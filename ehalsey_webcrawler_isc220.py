# -*- coding: utf-8 -*-
"""ehalsey-webcrawler-ISC220.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12gE1pEs4V3nv85PEc9TWA0Ckd0dFHhRJ

# **Python Web Crawler**
### Emma Halsey | ISC 220 | Fall 2025
This program searches for a query within a given webpage.

## 1) Verification
We need to ensure that the data can be read before going further.

### urlparse
`urlparse` works with URLs by splitting them into components. I used this to fetch the `robots.txt` of the webpage I wanted.
"""

from urllib.parse import urlparse

def fetch_robots_txt(url):
  try:
    parsed_url = urlparse(url)
    robots_url = f"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt"
  except ValueError:
    print(f"Invalid URL. Please try again.")
    return
  return robots_url

"""### robotparser
`robotparser` parses and evaluates the permissions in `robots.txt` files to verify that they can be read with the web crawler.
"""

from urllib import robotparser

def can_fetch(robots_url, page_url, user_agent="*"):
  rp = robotparser.RobotFileParser()
  rp.set_url(robots_url)
  rp.read()
  return rp.can_fetch(user_agent, page_url)

"""## 2) Data Parsing
After getting the url with `urlparse` and verifying permissions with `robotparser`, it is time to extract the data using `requests`, `BeautifulSoup`, and `urljoin`.

#### Extract metadata
`BeautifulSoup` and `Selenium` are two `Python` libraries commonly used for web crawling and scraping, including *metadata*.

*Metadata* is the overarching information about the website, including the **URL**, **Title**, and **Description**. There is other information in metadata, such as **Author**, **Date Created**, and **Date Modified**, but I chose not to include those in this webcrawler since they are irrelevant to the end user for this program's purpose.
"""

import requests
from bs4 import BeautifulSoup

def extract_data(soup, page_url):
  print(f"Getting site information...")
  print(f"\n")
  page_title = soup.title.string if soup.title else "No Title Found"
  meta_description = soup.find('meta', attrs={'name':'description'})
  meta_description = meta_description['content'] if meta_description else "No Description Found"

  print(f"SITE INFORMATION")
  print(f"URL: {page_url}")
  print(f"Title: {page_title}")
  print(f"Description: {meta_description}")
  print("\n")

"""#### Extract associated links from webpage

Again, you can see that `BeautifulSoup` was used to extract all the hyperlinks located within the base URL's webpage.
"""

from urllib.parse import urljoin

def extract_links(soup, base_url):
  discovered_links = []   # adding all found links to a list which will be used later in many situations.
  for link in soup.find_all('a', href=True):
    href = link.get('href')
    absolute_url = urljoin(base_url, href)
    discovered_links.append(absolute_url)
  return discovered_links

"""#### Categorize links - setup for URL frontier
Determines whether the links are internal or external to the main URL.
"""

def categorize_links(base_url, discovered_links):
  base_domain = urlparse(base_url).netloc
  same_domain_links = []
  external_links = []

  for link in discovered_links:
    link_domain = urlparse(link).netloc
    if link_domain == base_domain:
      same_domain_links.append(link)
    else:
      external_links.append(link)

  return same_domain_links, external_links

"""#### URL Frontier
Ensures URLs from the same domain as the initial URL are prioritized over external domains. For example, when searching http://oswego.edu, this prevents the webcrawler from searching Oswego's social media pages, or other Oswego webpages like the Alumni or Athletics page (which have a different domain).
"""

from collections import deque

class URLFrontier:
  def __init__(self):
    self.same_domain_queue = deque()  # High priority link
    self.external_queue = deque()     # Lower priority link

  def add_url(self, url, is_same_domain):
    if is_same_domain:
      self.same_domain_queue.append(url)
    else:
      self.external_queue.append(url)

  def get_next_url(self):
    if self.same_domain_queue:
      return self.same_domain_queue.popleft()
    elif self.external_queue:
      return self.external_queue.popleft()
    return None

  def is_empty(self):
    return not self.same_domain_queue and not self.external_queue

"""#### Verify whether the query is located within the current webpage's HTML text
Uses `.lower()` to ensure queries will match even if capitalization is different.
"""

def query_present(query, text):
  print(f"Checking if '{query}' is in the webpage...")
  return query.lower() in text.lower()

"""## 3) Final Steps
Time to put it all together.

#### Wrap-up
Before running in the main cell, I put most of the work into a method called `crawl_page`, using all the methods I previously created.
"""

def crawl_page(url, query, query_found_urls):

  print(f"Extracting data from {url}...")
  try:
    response = requests.get(url) # try to read from url
    response.raise_for_status() # check for HTTP errors
  except requests.exceptions.RequestException as e:
    print(f"Error fetching URL {url}: {e}")
    return []

  # if it works, then parse the HTML content
  soup = BeautifulSoup(response.text, 'html.parser')

  # extract some metadata (title, description) - passing 'url'
  extract_data(soup, url)

  # call extract_links and store returned links
  discovered_links = extract_links(soup, url)

  # check if query is present in webpage, searching in response.text
  if query_present(query, response.text):
    print(f"'{query}' was found in the webpage.")
    query_found_urls.append(url) # add URL to list if query is found
  else:
    print(f"'{query}' is not present in the webpage.")
  print("\n")

  # Return the discovered links
  return discovered_links

"""#### Main Executable Cell
The main "method", or cell (as called in `Python` notebooks), putting everything together.

This cell gets the user input, conducts a breadth-first search (BFS) through the given webpage, and contains additional error checking to ensure nothing slips between the cracks. Finally, it prints out a list of the links within the main URL that contained the search query.
"""

# MAIN BLOCK

# initialize variables for BFS
url_frontier = URLFrontier()
visited_urls = set()
query_found_urls = []
MAX_CRAWL_COUNT = 25
crawled_count = 0

# get query and starting URL from user
print("Welcome to Emma's Web Crawler!")
print("\n")
query = input("Enter what you're searching for: ")
start_url = input("Enter the starting URL for the web crawl: ")

# Add the initial URL to the frontier, assuming it's a same-domain link to start
url_frontier.add_url(start_url, True)

# run breadth-first search (BFS)
while not url_frontier.is_empty() and crawled_count < MAX_CRAWL_COUNT:
  current_url = url_frontier.get_next_url() # Get next URL from frontier

  if current_url in visited_urls:
    continue

  visited_urls.add(current_url)
  crawled_count += 1

  print(f"\nCrawling URL {crawled_count}/{MAX_CRAWL_COUNT}: {current_url}")

  rb_txt_url = fetch_robots_txt(current_url)

  try:
    # error checking
    if not can_fetch(rb_txt_url, current_url):
      print(f"Permission to access {current_url} is denied by the site owner's robots.txt.")
      continue
  except ValueError:
    print(f"Invalid URL for {current_url}. Skipping permission check. Please enter your link skin the correct format, such as http://oswego.edu.")
    continue
  except Exception as e:
    print(f"An unknown error occurred during robots.txt check for {current_url}: {e}")
    continue

  discovered_links = crawl_page(current_url, query, query_found_urls)

  # Categorize and add discovered links to the frontier
  same_domain_links, external_links = categorize_links(start_url, discovered_links)

  for link in same_domain_links:
    if link not in visited_urls:
      url_frontier.add_url(link, True)
  for link in external_links:
    if link not in visited_urls:
      url_frontier.add_url(link, False)

print("\n--- Crawl Summary ---")
print(f"Total unique URLs crawled: {len(visited_urls)}")
print("URLs where the query was found:")
if query_found_urls:
  for u in query_found_urls:
    print(f"- {u}")
else:
  print("  None")

print("\nAll visited URLs:")
for u in visited_urls:
  print(f"- {u}")